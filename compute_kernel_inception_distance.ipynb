{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd0b826e582c4c6950429e4ab08d565c1942dc747a4b972bee2d4ad610561dc5dd8",
   "display_name": "Python 3.8.2 64-bit ('venv_repos': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from utils.utils import *\n",
    "from datetime import datetime\n",
    "from evaluation.train_inception_model import SpectrogramInception3\n",
    "from data.preprocessing import AudioProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluation.metrics.maximum_mean_discrepancy import mmd\n",
    "from data.audio_transforms import MelScale\n",
    "from data.loaders import get_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"footsteps_inception_model_best_2021-04-29.pt\",\n",
    "    \"comments\": \"inception trained on footsteps dataset\",\n",
    "    \"state_dict_path\": \"evaluation/inception_models/footsteps_inception_model_best_2021-04-29.pt\",\n",
    "    \n",
    "    \"real_samples_path\": \"../_data/footsteps_real\",\n",
    "    \"synth_samples_path\": \"../_data/footsteps_generated_23-04-2021_15h\",\n",
    "    \n",
    "    \"output_path\": \"evaluation\",\n",
    "    \"output_folder\": \"evaluation_metrics\",\n",
    "    \n",
    "    \"batch_size\": 20,\n",
    "\n",
    "    \"real_samples_loader_config\": {\n",
    "        \"dbname\": \"footsteps\",\n",
    "        \"data_path\": \"../_data/footsteps_real/\",\n",
    "        \"criteria\": {},\n",
    "        \"shuffle\": True,\n",
    "        \"tr_val_split\": 1.0\n",
    "    },\n",
    "\n",
    "    \"synth_samples_loader_config\": {\n",
    "        \"dbname\": \"footsteps\",\n",
    "        \"data_path\": \"../_data/footsteps_generated_23-04-2021_15h/\",\n",
    "        \"criteria\": {},\n",
    "        \"shuffle\": True,\n",
    "        \"tr_val_split\": 1.0\n",
    "    },\n",
    "    \n",
    "    \"transform_config\": {\n",
    "        \"transform\": \"stft\",\n",
    "        \"fade_out\": True,\n",
    "        \"fft_size\": 1024,\n",
    "        \"win_size\": 1024,\n",
    "        \"n_frames\": 64,\n",
    "        \"hop_size\": 256,\n",
    "        \"log\": False,\n",
    "        \"ifreq\": False,\n",
    "        \"sample_rate\": 16000,\n",
    "        \"audio_length\": 16000\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "state_dict_path = config['state_dict_path']\n",
    "output_path = mkdir_in_path(config['output_path'], config['output_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuring stft transform...\n",
      "Dataset ../_data/footsteps_real/_processed/footsteps_stft/footsteps_stft.pt exists. Reloading...\n",
      "Dataset ../_data/footsteps_generated_23-04-2021_15h/_processed/footsteps_stft/footsteps_stft.pt exists. Reloading...\n",
      "n_real_samples:  720\n",
      "n_synth_samples:  600\n",
      "Cuda not available. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "real_samples_loader_config = config['real_samples_loader_config']\n",
    "synth_samples_loader_config = config['synth_samples_loader_config']\n",
    "\n",
    "transform_config = config['transform_config']\n",
    "transform = transform_config['transform']\n",
    "\n",
    "dbname = real_samples_loader_config['dbname']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "processor = AudioProcessor(**transform_config)\n",
    "\n",
    "loader_module = get_data_loader(dbname)\n",
    "\n",
    "real_samples_loader = loader_module(name=dbname + '_' + transform, preprocessing=processor, **real_samples_loader_config)\n",
    "synth_samples_loader = loader_module(name=dbname + '_' + transform, preprocessing=processor, **synth_samples_loader_config)\n",
    "\n",
    "n_real_samples = len(real_samples_loader)\n",
    "print('n_real_samples: ', n_real_samples)\n",
    "n_synth_samples = len(synth_samples_loader)\n",
    "print('n_synth_samples: ', n_synth_samples)\n",
    "\n",
    "real_samples_data_loader = DataLoader(real_samples_loader,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "synth_samples_data_loader = DataLoader(synth_samples_loader,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "\n",
    "device = 'cuda' if GPU_is_available() else 'cpu'\n",
    "\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "inception_footsteps = SpectrogramInception3(state_dict['fc.weight'].shape[0], aux_logits=False)\n",
    "inception_footsteps.load_state_dict(state_dict)\n",
    "inception_footsteps = inception_footsteps.to(device)\n",
    "\n",
    "mel = MelScale(sample_rate=transform_config['sample_rate'],\n",
    "                fft_size=transform_config['fft_size'],\n",
    "                n_mel=transform_config.get('n_mel', 256),\n",
    "                rm_dc=True)\n",
    "mel = mel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch:  0\n",
      "batch:  1\n",
      "batch:  2\n",
      "batch:  3\n",
      "batch:  4\n",
      "batch:  5\n",
      "batch:  6\n",
      "batch:  7\n",
      "batch:  8\n",
      "batch:  9\n",
      "batch:  10\n",
      "batch:  11\n",
      "batch:  12\n",
      "batch:  13\n",
      "batch:  14\n",
      "batch:  15\n",
      "batch:  16\n",
      "batch:  17\n",
      "batch:  18\n",
      "batch:  19\n",
      "batch:  20\n",
      "batch:  21\n",
      "batch:  22\n",
      "batch:  23\n",
      "batch:  24\n",
      "batch:  25\n",
      "batch:  26\n",
      "batch:  27\n",
      "batch:  28\n",
      "batch:  29\n",
      "batch:  30\n",
      "batch:  31\n",
      "batch:  32\n",
      "batch:  33\n",
      "batch:  34\n",
      "batch:  35\n"
     ]
    }
   ],
   "source": [
    "real_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(real_samples_data_loader):\n",
    "        input, labels = data\n",
    "        input.to(device)\n",
    "        input = mel(input.float())\n",
    "        mag_input = F.interpolate(input[:, 0:1], (299, 299))\n",
    "        \n",
    "        preds = inception_footsteps(mag_input.float())\n",
    "        \n",
    "        real_logits.append(preds)\n",
    "\n",
    "        print('batch: ', batch_idx)\n",
    "    \n",
    "real_logits = torch.cat(real_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch:  0\n",
      "batch:  1\n",
      "batch:  2\n",
      "batch:  3\n",
      "batch:  4\n",
      "batch:  5\n",
      "batch:  6\n",
      "batch:  7\n",
      "batch:  8\n",
      "batch:  9\n",
      "batch:  10\n",
      "batch:  11\n",
      "batch:  12\n",
      "batch:  13\n",
      "batch:  14\n",
      "batch:  15\n",
      "batch:  16\n",
      "batch:  17\n",
      "batch:  18\n",
      "batch:  19\n",
      "batch:  20\n",
      "batch:  21\n",
      "batch:  22\n",
      "batch:  23\n",
      "batch:  24\n",
      "batch:  25\n",
      "batch:  26\n",
      "batch:  27\n",
      "batch:  28\n",
      "batch:  29\n"
     ]
    }
   ],
   "source": [
    "synth_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(synth_samples_data_loader):\n",
    "        input, labels = data\n",
    "        input.to(device)\n",
    "        input = mel(input.float())\n",
    "        mag_input = F.interpolate(input[:, 0:1], (299, 299))\n",
    "        \n",
    "        preds = inception_footsteps(mag_input.float())\n",
    "        \n",
    "        synth_logits.append(preds)\n",
    "\n",
    "        print('batch: ', batch_idx)\n",
    "    \n",
    "synth_logits = torch.cat(synth_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean_MMD:  0.011842087\nvar_MMD:  0.0\n"
     ]
    }
   ],
   "source": [
    "mmd_distance = []\n",
    "mmd_distance.append(mmd(real_logits, synth_logits))\n",
    "mean_MMD = np.mean(mmd_distance)\n",
    "var_MMD = np.std(mmd_distance)\n",
    "print('mean_MMD: ', mean_MMD)\n",
    "print('var_MMD: ', var_MMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'{output_path}/KID_{model_name}_{datetime.now().strftime(\"%d-%m-%y_%H_%M\")}.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(str(mean_MMD)+'\\n')\n",
    "    f.write(str(var_MMD))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}